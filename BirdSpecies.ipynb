{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g1lzjW5NJNT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"yashikota/birds-525-species-image-classification\")\n",
        "# print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "df = pd.DataFrame(ds['train'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "pYRt3PwXfv03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "for x in ds['train'].shuffle(seed=231).select(range(5)):\n",
        "  display(x[\"image\"])\n",
        "  print(\"Label: \", ds['train'].features['label'].int2str(x['label']))"
      ],
      "metadata": {
        "id": "NRf_xO1igqY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Label and Image for an American Wigeon\n",
        "display(ds[\"train\"][3924][\"image\"])\n",
        "display(ds['train'].features[\"label\"].int2str(ds['train'][3924]['label']))"
      ],
      "metadata": {
        "id": "tRwkDSr_QhZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying Features to see our classes\n",
        "display(ds['train'].features)"
      ],
      "metadata": {
        "id": "TpaaGdaLQ7sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for corrupted images by image link is None\n",
        "bad_images = []\n",
        "\n",
        "for i in range(len(ds['train'])):\n",
        "  image = ds['train'][i]['image']\n",
        "  if image is None:\n",
        "    bad_images.append(i)\n",
        "\n",
        "print(f\"Total number of corrupt/null images: {len(bad_images)}\")\n",
        "print(\"First Bad Ones: \", bad_images[:25])"
      ],
      "metadata": {
        "id": "Wj6M7PZyhKLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for bird images that are not the same size as the defaulted 224x224\n",
        "size_unmatched = []\n",
        "for i in range(len(ds['train'])):\n",
        "  width, height = ds['train'][i]['image'].size\n",
        "  if width != 224 or height != 224:\n",
        "    size_unmatched.append(i)\n",
        "print(f\"Amount of images not 224x224:{len(size_unmatched)}\")\n",
        "print(size_unmatched[:20])"
      ],
      "metadata": {
        "id": "TYzDEua5kgxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding size differences by checking a few of the images not our default size\n",
        "for idx in size_unmatched[:10]:\n",
        "  display(ds['train'][idx]['image'])"
      ],
      "metadata": {
        "id": "oKXuyooQklS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing Images using Hugging Face Map and PILLOW Resampling\n",
        "def resize_image(examples):\n",
        "  image = examples[\"image\"]\n",
        "  if image.size != (224,224):\n",
        "    image = image.resize((224,224), Image.Resampling.BILINEAR)\n",
        "  examples[\"image\"] = image\n",
        "  return examples\n",
        "\n",
        "ds = ds.map(resize_image)\n",
        "\n",
        "size_unmatched = []\n",
        "for i in range(len(ds['train'])):\n",
        "  width, height = ds['train'][i]['image'].size\n",
        "  if width != 224 or height != 224:\n",
        "    size_unmatched.append(i)\n",
        "print(f\"Amount of images not 224x224:{len(size_unmatched)}\")\n",
        "print(size_unmatched[:20])"
      ],
      "metadata": {
        "id": "Xx-rDAk9YtMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "e30OQTYqk6YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_image = ds['train'][0]['image']\n",
        "\n",
        "display(first_image)\n",
        "\n",
        "image_size = first_image.size\n",
        "print(f\"Image size (width, height): {image_size}\")\n",
        "\n",
        "image_format = first_image.format\n",
        "print(f\"Image format: {image_format}\")\n",
        "\n",
        "image_mode = first_image.mode\n",
        "print(f\"Image mode: {image_mode}\")"
      ],
      "metadata": {
        "id": "vKwoFfz2lzVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = ResNet18_Weights.DEFAULT\n",
        "model = models.resnet18(weights=weights)\n",
        "model.fc = torch.nn.Identity()\n",
        "model.eval()\n",
        "transform = weights.transforms()\n",
        "\n",
        "def get_embedding(img):\n",
        " x = transform(img).unsqueeze(0)\n",
        " with torch.no_grad():\n",
        "    emb = model(x).squeeze().numpy()\n",
        " return emb"
      ],
      "metadata": {
        "id": "zxVHKc0Pl2gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "image1 = ds['train'][0]['image']\n",
        "image2 = ds['train'][3]['image']\n",
        "\n",
        "print(\"Image 1:\")\n",
        "display(image1)\n",
        "print(\"\\nImage 2:\")\n",
        "display(image2)\n",
        "\n",
        "\n",
        "img1 = get_embedding(image1)\n",
        "img2 = get_embedding(image2)\n",
        "\n",
        "\n",
        "img1 = img1.reshape(1, -1)\n",
        "img2 = img2.reshape(1, -1)\n",
        "\n",
        "similarity = cosine_similarity(img1, img2)[0][0]\n",
        "\n",
        "print(f\"\\nSimilarity between the first two images: {similarity}\")\n",
        "\n",
        "if similarity > 0.90:\n",
        "    print(\"duplicates\")\n",
        "else:\n",
        "    print(\"not duplicates.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CQTSmWltl47t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class imbalances\n",
        "#making a dictionary of species names and how many pictures there are of each species to filter through later\n",
        "#only done for training data since we are just training the model\n",
        "\n",
        "#print(ds['train'].features) #dictionary of names from the label\n",
        "from collections import Counter #counts how many there are of each label\n",
        "values=ds[\"train\"][\"label\"]\n",
        "counts=Counter(values)\n",
        "#print(counts) #how much there are of each value, key associated with a certain species\n",
        "labels=ds[\"train\"].features[\"label\"].names #getting the species names\n",
        "countsSpecies={labels[i]: c for i, c in counts.items()} #makes a dictionary of the species and how many images there are in alphabetical order\n",
        "#print(countsSpecies)"
      ],
      "metadata": {
        "id": "AAcC_H5Wzv01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning weights to different classes, best for datasets with medium levels of imbalance (eg. here)\n",
        "#using cross entropy to have the model weigh classes with less samples more\n",
        "#undersampling is not preferred due to the risk of permanently removing important data from the dataset\n",
        "\n",
        "#for PyTorch\n",
        "import torch\n",
        "\n",
        "classCounts=torch.tensor([counts[i] for i in range(len(labels))], dtype=torch.float) #creates a list of how many there are per species alphabetically\n",
        "\n",
        "n=classCounts.sum() #formula for computing the class weights\n",
        "c=len(classCounts)\n",
        "classWeights=n/(c*classCounts)\n",
        "#print(classWeights)"
      ],
      "metadata": {
        "id": "DJZIe-9pz1uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the loss function with these weights to properly evaluate the accuracy after training\n",
        "import torch.nn as nn\n",
        "criterion=nn.CrossEntropyLoss(weight=classWeights)\n",
        "#all of the above goes before creating the model and training it\n",
        "#with this, running the loss function weighs mistakes more heavily on the species with less pictures than speices with more pictures"
      ],
      "metadata": {
        "id": "mKGotoG0z4DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNet model training starts here\n",
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "2DPc5G8TiIoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix class weights (class 381 is empty)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "labeles = ds['train'].features['label'].names\n",
        "label_values = ds['train']['label']\n",
        "counts = Counter(label_values)\n",
        "\n",
        "classCounts = torch.tensor([counts.get(i,0) for i in range(len(labels))], dtype=torch.float)\n",
        "print(f'original classcounts: {classCounts[381]}')\n",
        "\n",
        "classCounts[classCounts == 0] = 1\n",
        "print(f'fixed classcounts: {classCounts[381]}')\n",
        "\n",
        "n = classCounts.sum()\n",
        "c = len(labels)\n",
        "classWeights = n / (c * classCounts)\n",
        "\n",
        "print(f'Any infinite weights: {torch.isinf(classWeights).any()}')"
      ],
      "metadata": {
        "id": "1Oi9MuYtGYid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dataset to pytorch format\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # use predefined imagenet values\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# custom class\n",
        "class BirdDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "      self.dataset = hf_dataset\n",
        "      self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      item = self.dataset[idx]\n",
        "      image = item['image']\n",
        "      label = item['label']\n",
        "      if self.transform:\n",
        "        image = self.transform(image)\n",
        "      return image, label\n",
        "\n",
        "train_dataset = BirdDataset(ds['train'], transform=transform)\n",
        "val_dataset = BirdDataset(ds['validation'], transform=transform)\n",
        "test_dataset = BirdDataset(ds['test'], transform=transform)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "N1zRjC1liKUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders (help us batch process and randomize order)\n",
        "\n",
        "BATCH_SIZE  = 32\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "print(f\"Number of training batches: {len(train_loader)}\")"
      ],
      "metadata": {
        "id": "o0OVy3FEBPTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "\n",
        "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=526)\n",
        "device = torch.device('cuda')\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())} parameters\")"
      ],
      "metadata": {
        "id": "tnlgZe-pHWKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up training weights\n",
        "\n",
        "classWeights = classWeights.to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=classWeights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        ")"
      ],
      "metadata": {
        "id": "JNBVQ4WSJJmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training definitions\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for images, labels in tqdm(train_loader, desc=\"training\"):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "  epoch_loss = running_loss/len(train_loader)\n",
        "  epoch_acc = 100. * correct / total\n",
        "  return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for images, labels in tqdm(val_loader, desc=\"validation\"):\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          running_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += labels.size(0)\n",
        "          correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "      epoch_loss = running_loss / len(val_loader)\n",
        "      epoch_acc = 100. * correct / total\n",
        "      return epoch_loss, epoch_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "q5AitLwHLMRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run training\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "best_val_acc = 0.0\n",
        "print('Starting training: \\n')\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "  train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "  print(f\"Train loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "  val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "  print(f\"Val loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "  if val_acc > best_val_acc:\n",
        "    best_val_acc = val_acc\n",
        "    torch.save(model.state_dict(), 'best_efficientnet_b0_birds.pth')\n",
        "    print(f\"Training complete, best val accuracy: {best_val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "3jQpxdLURF9w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}