{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g1lzjW5NJNT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"yashikota/birds-525-species-image-classification\")\n",
        "# print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "df = pd.DataFrame(ds['train'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "pYRt3PwXfv03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "for x in ds['train'].shuffle(seed=231).select(range(5)):\n",
        "  display(x[\"image\"])\n",
        "  print(\"Label: \", ds['train'].features['label'].int2str(x['label']))"
      ],
      "metadata": {
        "id": "NRf_xO1igqY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Label and Image for an American Wigeon\n",
        "display(ds[\"train\"][3924][\"image\"])\n",
        "display(ds['train'].features[\"label\"].int2str(ds['train'][3924]['label']))"
      ],
      "metadata": {
        "id": "tRwkDSr_QhZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying Features to see our classes\n",
        "display(ds['train'].features)"
      ],
      "metadata": {
        "id": "TpaaGdaLQ7sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for corrupted images by image link is None\n",
        "bad_images = []\n",
        "\n",
        "for i in range(len(ds['train'])):\n",
        "  image = ds['train'][i]['image']\n",
        "  if image is None:\n",
        "    bad_images.append(i)\n",
        "\n",
        "print(f\"Total number of corrupt/null images: {len(bad_images)}\")\n",
        "print(\"First Bad Ones: \", bad_images[:25])"
      ],
      "metadata": {
        "id": "Wj6M7PZyhKLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for bird images that are not the same size as the defaulted 224x224\n",
        "size_unmatched = []\n",
        "for i in range(len(ds['train'])):\n",
        "  width, height = ds['train'][i]['image'].size\n",
        "  if width != 224 or height != 224:\n",
        "    size_unmatched.append(i)\n",
        "print(f\"Amount of images not 224x224:{len(size_unmatched)}\")\n",
        "print(size_unmatched[:20])"
      ],
      "metadata": {
        "id": "TYzDEua5kgxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding size differences by checking a few of the images not our default size\n",
        "for idx in size_unmatched[:10]:\n",
        "  display(ds['train'][idx]['image'])"
      ],
      "metadata": {
        "id": "oKXuyooQklS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing Images using Hugging Face Map and PILLOW Resampling\n",
        "def resize_image(examples):\n",
        "  image = examples[\"image\"]\n",
        "  if image.size != (224,224):\n",
        "    image = image.resize((224,224), Image.Resampling.BILINEAR)\n",
        "  examples[\"image\"] = image\n",
        "  return examples\n",
        "\n",
        "ds = ds.map(resize_image)\n",
        "\n",
        "size_unmatched = []\n",
        "for i in range(len(ds['train'])):\n",
        "  width, height = ds['train'][i]['image'].size\n",
        "  if width != 224 or height != 224:\n",
        "    size_unmatched.append(i)\n",
        "print(f\"Amount of images not 224x224:{len(size_unmatched)}\")\n",
        "print(size_unmatched[:20])"
      ],
      "metadata": {
        "id": "Xx-rDAk9YtMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "e30OQTYqk6YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_image = ds['train'][0]['image']\n",
        "\n",
        "display(first_image)\n",
        "\n",
        "image_size = first_image.size\n",
        "print(f\"Image size (width, height): {image_size}\")\n",
        "\n",
        "image_format = first_image.format\n",
        "print(f\"Image format: {image_format}\")\n",
        "\n",
        "image_mode = first_image.mode\n",
        "print(f\"Image mode: {image_mode}\")"
      ],
      "metadata": {
        "id": "vKwoFfz2lzVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = ResNet18_Weights.DEFAULT\n",
        "model = models.resnet18(weights=weights)\n",
        "model.fc = torch.nn.Identity()\n",
        "model.eval()\n",
        "transform = weights.transforms()\n",
        "\n",
        "def get_embedding(img):\n",
        " x = transform(img).unsqueeze(0)\n",
        " with torch.no_grad():\n",
        "    emb = model(x).squeeze().numpy()\n",
        " return emb"
      ],
      "metadata": {
        "id": "zxVHKc0Pl2gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "image1 = ds['train'][0]['image']\n",
        "image2 = ds['train'][3]['image']\n",
        "\n",
        "print(\"Image 1:\")\n",
        "display(image1)\n",
        "print(\"\\nImage 2:\")\n",
        "display(image2)\n",
        "\n",
        "\n",
        "img1 = get_embedding(image1)\n",
        "img2 = get_embedding(image2)\n",
        "\n",
        "\n",
        "img1 = img1.reshape(1, -1)\n",
        "img2 = img2.reshape(1, -1)\n",
        "\n",
        "similarity = cosine_similarity(img1, img2)[0][0]\n",
        "\n",
        "print(f\"\\nSimilarity between the first two images: {similarity}\")\n",
        "\n",
        "if similarity > 0.90:\n",
        "    print(\"duplicates\")\n",
        "else:\n",
        "    print(\"not duplicates.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CQTSmWltl47t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class imbalances\n",
        "#making a dictionary of species names and how many pictures there are of each species to filter through later\n",
        "#only done for training data since we are just training the model\n",
        "\n",
        "#print(ds['train'].features) #dictionary of names from the label\n",
        "from collections import Counter #counts how many there are of each label\n",
        "values=ds[\"train\"][\"label\"]\n",
        "counts=Counter(values)\n",
        "#print(counts) #how much there are of each value, key associated with a certain species\n",
        "labels=ds[\"train\"].features[\"label\"].names #getting the species names\n",
        "countsSpecies={labels[i]: c for i, c in counts.items()} #makes a dictionary of the species and how many images there are in alphabetical order\n",
        "#print(countsSpecies)"
      ],
      "metadata": {
        "id": "AAcC_H5Wzv01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning weights to different classes, best for datasets with medium levels of imbalance (eg. here)\n",
        "#using cross entropy to have the model weigh classes with less samples more\n",
        "#undersampling is not preferred due to the risk of permanently removing important data from the dataset\n",
        "\n",
        "#for PyTorch\n",
        "import torch\n",
        "\n",
        "classCounts=torch.tensor([counts[i] for i in range(len(labels))], dtype=torch.float) #creates a list of how many there are per species alphabetically\n",
        "\n",
        "n=classCounts.sum() #formula for computing the class weights\n",
        "c=len(classCounts)\n",
        "classWeights=n/(c*classCounts)\n",
        "#print(classWeights)"
      ],
      "metadata": {
        "id": "DJZIe-9pz1uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the loss function with these weights to properly evaluate the accuracy after training\n",
        "import torch.nn as nn\n",
        "criterion=nn.CrossEntropyLoss(weight=classWeights)\n",
        "#all of the above goes before creating the model and training it\n",
        "#with this, running the loss function weighs mistakes more heavily on the species with less pictures than speices with more pictures"
      ],
      "metadata": {
        "id": "mKGotoG0z4DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNet model training starts here\n",
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "2DPc5G8TiIoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix class weights (class 381 is empty)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "labeles = ds['train'].features['label'].names\n",
        "label_values = ds['train']['label']\n",
        "counts = Counter(label_values)\n",
        "\n",
        "classCounts = torch.tensor([counts.get(i,0) for i in range(len(labels))], dtype=torch.float)\n",
        "print(f'original classcounts: {classCounts[381]}')\n",
        "\n",
        "classCounts[classCounts == 0] = 1\n",
        "print(f'fixed classcounts: {classCounts[381]}')\n",
        "\n",
        "n = classCounts.sum()\n",
        "c = len(labels)\n",
        "classWeights = n / (c * classCounts)\n",
        "\n",
        "print(f'Any infinite weights: {torch.isinf(classWeights).any()}')"
      ],
      "metadata": {
        "id": "1Oi9MuYtGYid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out class 381 completely (has 0 training images, but has images in validation set)\n",
        "\n",
        "print(\"Original dataset sizes:\")\n",
        "print(f\"  Train: {len(ds['train'])}\")\n",
        "print(f\"  Validation: {len(ds['validation'])}\")\n",
        "print(f\"  Test: {len(ds['test'])}\")\n",
        "\n",
        "# Filter out class 381 from all splits\n",
        "ds['train'] = ds['train'].filter(lambda x: x['label'] != 381)\n",
        "ds['validation'] = ds['validation'].filter(lambda x: x['label'] != 381)\n",
        "ds['test'] = ds['test'].filter(lambda x: x['label'] != 381)\n",
        "\n",
        "print(\"\\nAfter removing PARAKEET AUKLET (class 381):\")\n",
        "print(f\"  Train: {len(ds['train'])} (-{84635 - len(ds['train'])} images)\")\n",
        "print(f\"  Validation: {len(ds['validation'])} (-{2625 - len(ds['validation'])} images)\")\n",
        "print(f\"  Test: {len(ds['test'])} (-{2625 - len(ds['test'])} images)\")\n",
        "print(\"\\n✓ Class 381 removed from all splits\")"
      ],
      "metadata": {
        "id": "eeRhWB7BBzXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dataset to pytorch format\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # use predefined imagenet values\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# custom class\n",
        "class BirdDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "      self.dataset = hf_dataset\n",
        "      self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      item = self.dataset[idx]\n",
        "      image = item['image']\n",
        "      label = item['label']\n",
        "      if self.transform:\n",
        "        image = self.transform(image)\n",
        "      return image, label\n",
        "\n",
        "train_dataset = BirdDataset(ds['train'], transform=transform)\n",
        "val_dataset = BirdDataset(ds['validation'], transform=transform)\n",
        "test_dataset = BirdDataset(ds['test'], transform=transform)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "N1zRjC1liKUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders (help us batch process and randomize order)\n",
        "\n",
        "BATCH_SIZE  = 32\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "print(f\"Number of training batches: {len(train_loader)}\")"
      ],
      "metadata": {
        "id": "o0OVy3FEBPTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "\n",
        "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=526)\n",
        "device = torch.device('cuda')\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())} parameters\")"
      ],
      "metadata": {
        "id": "tnlgZe-pHWKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up training weights\n",
        "\n",
        "classWeights = classWeights.to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=classWeights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        ")"
      ],
      "metadata": {
        "id": "JNBVQ4WSJJmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training definitions\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for images, labels in tqdm(train_loader, desc=\"training\"):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "  epoch_loss = running_loss/len(train_loader)\n",
        "  epoch_acc = 100. * correct / total\n",
        "  return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for images, labels in tqdm(val_loader, desc=\"validation\"):\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          running_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += labels.size(0)\n",
        "          correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "      epoch_loss = running_loss / len(val_loader)\n",
        "      epoch_acc = 100. * correct / total\n",
        "      return epoch_loss, epoch_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "q5AitLwHLMRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run training\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "best_val_acc = 0.0\n",
        "print('Starting training: \\n')\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "  train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "  print(f\"Train loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "  val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "  print(f\"Val loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "  if val_acc > best_val_acc:\n",
        "    best_val_acc = val_acc\n",
        "    torch.save(model.state_dict(), 'best_efficientnet_b0_birds.pth')\n",
        "    print(f\"Training complete, best val accuracy: {best_val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "3jQpxdLURF9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze results\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "def analyze_confusion(dataset, top_n=15):\n",
        "    # find which species pairs get confused most often\n",
        "\n",
        "    print(\"Analyzing errors on validation set...\")\n",
        "    model.eval()\n",
        "\n",
        "    confusion_pairs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(range(len(dataset))):\n",
        "            image, true_label = dataset[idx]\n",
        "            image = image.unsqueeze(0).to(device)\n",
        "\n",
        "            output = model(image)\n",
        "            _, predicted = output.max(1)\n",
        "            pred_label = predicted.item()\n",
        "\n",
        "            # only track errors\n",
        "            if pred_label != true_label:\n",
        "                confusion_pairs.append((true_label, pred_label))\n",
        "\n",
        "    # count most common confusions\n",
        "    confusion_counts = Counter(confusion_pairs)\n",
        "    top_confusions = confusion_counts.most_common(top_n)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TOP {top_n} MOST CONFUSED SPECIES PAIRS\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    for i, ((true_idx, pred_idx), count) in enumerate(top_confusions, 1):\n",
        "        true_name = labels[true_idx]\n",
        "        pred_name = labels[pred_idx]\n",
        "        print(f\"{i}. {count} times: '{true_name}' → predicted as → '{pred_name}'\")\n",
        "\n",
        "    return top_confusions\n",
        "\n",
        "top_errors = analyze_confusion(val_dataset, top_n=15)"
      ],
      "metadata": {
        "id": "eNjWkP0iAFdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visually show species that were confused\n",
        "\n",
        "def show_top_confusions_comparison(dataset, top_errors, num_pairs=3):\n",
        "\n",
        "    fig, axes = plt.subplots(num_pairs, 2, figsize=(12, 5*num_pairs))\n",
        "\n",
        "    for pair_idx in range(num_pairs):\n",
        "        true_idx, pred_idx = top_errors[pair_idx][0]\n",
        "        count = top_errors[pair_idx][1]\n",
        "\n",
        "        true_name = labels[true_idx]\n",
        "        pred_name = labels[pred_idx]\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"CONFUSION #{pair_idx+1}: {count} times\")\n",
        "        print(f\"TRUTH: {true_name}\")\n",
        "        print(f\"MODEL PREDICTED: {pred_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # find one example of this confusion\n",
        "        found = False\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for idx in range(len(dataset)):\n",
        "                image, label = dataset[idx]\n",
        "                if label == true_idx:\n",
        "                    image_batch = image.unsqueeze(0).to(device)\n",
        "                    output = model(image_batch)\n",
        "                    _, predicted = output.max(1)\n",
        "\n",
        "                    if predicted.item() == pred_idx:\n",
        "                        confused_image = dataset.dataset[idx]['image']\n",
        "                        found = True\n",
        "                        break\n",
        "\n",
        "        if not found:\n",
        "            print(\"No example found for this pair\")\n",
        "            continue\n",
        "\n",
        "        # LEFT: Show what it actually is (with a correct example)\n",
        "        ax_left = axes[pair_idx, 0] if num_pairs > 1 else axes[0]\n",
        "\n",
        "        # find a correct example of the true species\n",
        "        correct_example = None\n",
        "        with torch.no_grad():\n",
        "            for idx in range(len(dataset)):\n",
        "                image, label = dataset[idx]\n",
        "                if label == true_idx:\n",
        "                    image_batch = image.unsqueeze(0).to(device)\n",
        "                    output = model(image_batch)\n",
        "                    _, predicted = output.max(1)\n",
        "\n",
        "                    if predicted.item() == true_idx:\n",
        "                        correct_example = dataset.dataset[idx]['image']\n",
        "                        break\n",
        "\n",
        "        if correct_example is not None:\n",
        "            ax_left.imshow(correct_example)\n",
        "        else:\n",
        "            ax_left.imshow(confused_image)\n",
        "\n",
        "        ax_left.axis('off')\n",
        "        ax_left.set_title(f\"✓ {true_name}\\n(should've guessed)\",\n",
        "                         fontsize=12, color='green', weight='bold', pad=10)\n",
        "        ax_left.spines['top'].set_color('green')\n",
        "        ax_left.spines['bottom'].set_color('green')\n",
        "        ax_left.spines['left'].set_color('green')\n",
        "        ax_left.spines['right'].set_color('green')\n",
        "        ax_left.spines['top'].set_linewidth(5)\n",
        "        ax_left.spines['bottom'].set_linewidth(5)\n",
        "        ax_left.spines['left'].set_linewidth(5)\n",
        "        ax_left.spines['right'].set_linewidth(5)\n",
        "\n",
        "        # RIGHT: Show what model wrongly thinks it is\n",
        "        ax_right = axes[pair_idx, 1] if num_pairs > 1 else axes[1]\n",
        "        ax_right.imshow(confused_image)\n",
        "        ax_right.axis('off')\n",
        "        ax_right.set_title(f\"✗ MODEL SAYS: {pred_name}\\n(wrong guess)\",\n",
        "                          fontsize=12, color='red', weight='bold', pad=10)\n",
        "        ax_right.spines['top'].set_color('red')\n",
        "        ax_right.spines['bottom'].set_color('red')\n",
        "        ax_right.spines['left'].set_color('red')\n",
        "        ax_right.spines['right'].set_color('red')\n",
        "        ax_right.spines['top'].set_linewidth(5)\n",
        "        ax_right.spines['bottom'].set_linewidth(5)\n",
        "        ax_right.spines['left'].set_linewidth(5)\n",
        "        ax_right.spines['right'].set_linewidth(5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('top_3_confusions.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"\\n saved as 'top_3_confusions.png'\")\n",
        "\n",
        "show_top_confusions_comparison(val_dataset, top_errors, num_pairs=3)"
      ],
      "metadata": {
        "id": "_mfkVQhUAgHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import timm"
      ],
      "metadata": {
        "id": "q10ZPg6w4u0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #imagenet\n",
        "])"
      ],
      "metadata": {
        "id": "-iCMkCzv45bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModelImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item['image']\n",
        "        label = item['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "nbSNBJSw5hux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BaseModelImageDataset(ds['train'], transform=transform)\n",
        "val_dataset = BaseModelImageDataset(ds['validation'], transform=transform)"
      ],
      "metadata": {
        "id": "CzFEozBg5pIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "k4ieFURM5wbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(ds['train'].features['label'].names)\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "AegbEw5u5zTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "SE3a4Iuy55Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    print(f\"\\nEpoch {epoch + 1}\")\n",
        "    model.train()\n",
        "    total, correct, loss_total = 0, 0, 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        loss_total += loss.item()\n",
        "\n",
        "    print(f\"Train Loss: {loss_total / len(train_loader):.4f} | Train Acc: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f\"Val Acc: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "t_6VnHwX6vPX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}