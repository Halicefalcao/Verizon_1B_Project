{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8g1lzjW5NJNT"},"outputs":[],"source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"yashikota/birds-525-species-image-classification\")\n","# print(ds)"]},{"cell_type":"code","source":["import pandas as pd\n","from PIL import Image\n","df = pd.DataFrame(ds['train'])\n","df.head()"],"metadata":{"id":"pYRt3PwXfv03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display\n","for x in ds['train'].shuffle(seed=231).select(range(5)):\n","  display(x[\"image\"])\n","  print(\"Label: \", ds['train'].features['label'].int2str(x['label']))"],"metadata":{"id":"NRf_xO1igqY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Checking Label and Image for an American Wigeon\n","display(ds[\"train\"][3924][\"image\"])\n","display(ds['train'].features[\"label\"].int2str(ds['train'][3924]['label']))"],"metadata":{"id":"tRwkDSr_QhZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Displaying Features to see our classes\n","display(ds['train'].features)"],"metadata":{"id":"TpaaGdaLQ7sc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking for corrupted images by image link is None\n","bad_images = []\n","\n","for i in range(len(ds['train'])):\n","  image = ds['train'][i]['image']\n","  if image is None:\n","    bad_images.append(i)\n","\n","print(f\"Total number of corrupt/null images: {len(bad_images)}\")\n","print(\"First Bad Ones: \", bad_images[:25])"],"metadata":{"id":"Wj6M7PZyhKLQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking for bird images that are not the same size as the defaulted 224x224\n","size_unmatched = []\n","for i in range(len(ds['train'])):\n","  width, height = ds['train'][i]['image'].size\n","  if width != 224 or height != 224:\n","    size_unmatched.append(i)\n","print(f\"Amount of images not 224x224:{len(size_unmatched)}\")\n","print(size_unmatched[:20])"],"metadata":{"id":"TYzDEua5kgxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Understanding size differences by checking a few of the images not our default size\n","for idx in size_unmatched[:10]:\n","  display(ds['train'][idx]['image'])"],"metadata":{"id":"oKXuyooQklS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Resizing Images using Hugging Face Map and PILLOW Resampling\n","def resize_image(examples):\n","  image = examples[\"image\"]\n","  if image.size != (224,224):\n","    image = image.resize((224,224), Image.Resampling.BILINEAR)\n","  examples[\"image\"] = image\n","  return examples\n","\n","ds = ds.map(resize_image)\n","\n","size_unmatched = []\n","for i in range(len(ds['train'])):\n","  width, height = ds['train'][i]['image'].size\n","  if width != 224 or height != 224:\n","    size_unmatched.append(i)\n","print(f\"Amount of images not 224x224:{len(size_unmatched)}\")\n","print(size_unmatched[:20])"],"metadata":{"id":"Xx-rDAk9YtMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install faiss-cpu datasets transformers torch torchvision tqdm\n","\n","import numpy as np\n","import torch\n","from datasets import load_dataset\n","from transformers import CLIPProcessor, CLIPModel\n","from tqdm import tqdm\n","from collections import defaultdict\n","import faiss\n","import os"],"metadata":{"id":"e30OQTYqk6YA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds = load_dataset(\"yashikota/birds-525-species-image-classification\", split=\"train\")\n","print(f\"Dataset loaded: {len(ds)} images\")\n","# Running on GPU is much faster than CPU\n","# checks if GPU is available if yes then it switches to GPU for computation and then stores the embedding in CPU ( used for storage)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)#Using a pretrained CLIP model by OpenAi\n","#CLIP looks at the image and produces a vector like it gives a long list of numbers for that image\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","model.eval()\n","\n","#this function uses the CLIP model to convert images into numbers in batches\n","def get_embeddings_batched(images, batch_size=32):\n","    all_embeds = [] #used stores numerical representations for all images\n","    for i in tqdm(range(0, len(images), batch_size), desc=\"Embedding batches\"):\n","        batch = images[i:i+batch_size]#images are processed in batches\n","        inputs = processor(images=batch, return_tensors=\"pt\").to(device)#uses CLIP processor to turn the images into Pytorch tensors\n","        with torch.no_grad():\n","            embeds = model.get_image_features(**inputs)# converts images into vectors\n","            embeds = embeds / embeds.norm(dim=-1, keepdim=True)# makes the vector length of each vector to 1\n","        all_embeds.append(embeds.cpu())# moves embedding back to cpu to save\n","    return torch.cat(all_embeds, dim=0).numpy().astype(\"float32\")\n","\n","embeddings = get_embeddings_batched(ds[\"image\"], batch_size=64)\n","print(f\"Embeddings shape: {embeddings.shape}\")\n","\n","# here comparing the embeddings and checking for similarity\n","dim = embeddings.shape[1]# gets the size of each embedding vector\n","index = faiss.IndexFlatIP(dim)# index measures similarity by dot product\n","index.add(embeddings)# puts all embeddings into the index\n","print(f\"FAISS index built with {index.ntotal} embeddings.\")\n","\n","#finds k nearest neighbors for every image\n","k = 5 # for every image 5 neighbours\n","D, I = index.search(embeddings, k)# D = similarity scores, I = neighbor indices\n","threshold = 0.92\n","\n","#filtering pairs and keeping only neighbors with similarity above 0.95\n","pairs = np.argwhere(D[:,1:] > threshold)\n","graph = defaultdict(set)\n","#here building a graph of similar images where a node is image and an edge connects two images if they are very similar which is above threshold\n","for row, col in pairs:\n","    neighbor = I[row, col]\n","    graph[row].add(neighbor)\n","    graph[neighbor].add(row)\n","\n","#Grouping duplicates\n","visited = set()\n","components = []#list of all duplicate groups\n","for node in range(len(embeddings)):\n","  #goes through all images  and traverse the graph using stack (DPS)\n","    if node not in visited and graph[node]:\n","        stack = [node]\n","        comp = []#stores duplicate image pair\n","        while stack:\n","            n = stack.pop()\n","            if n not in visited:\n","                visited.add(n)\n","                comp.append(n)\n","                stack.extend(graph[n])# visit all connected neighbors\n","        if len(comp) > 1:\n","            components.append(comp) #if group has duplicates saves it\n","\n","print(f\"Found {len(components)} duplicate groups\")\n","\n","# Keep the first image of each duplicate group, remove others\n","remove = {i for comp in components for i in comp[1:]}\n","keep = [i for i in range(len(ds)) if i not in remove]\n","\n","#Two new datasets one clean and one with duplicates\n","cleaned_ds = ds.select(keep)\n","duplicates_ds = ds.select(list(remove))\n","\n","#Saving the dataset into disk so we can access it later\n","os.makedirs(\"cleaned_data\", exist_ok=True)#creates a folder cleaned_data to store the datasets\n","cleaned_path = \"cleaned_data/cleaned_birds\"\n","dup_path = \"cleaned_data/duplicates_birds\"\n","cleaned_ds.save_to_disk(cleaned_path)\n","duplicates_ds.save_to_disk(dup_path)\n","\n","print(\"\\nCleaning done\")\n","print(f\"Original size: {len(ds)}\")\n","print(f\"Cleaned dataset size: {len(cleaned_ds)}\")\n","print(f\"Duplicates removed: {len(duplicates_ds)}\")\n","print(f\"Percentage removed: {100*len(duplicates_ds)/len(ds):.2f}%\")"],"metadata":{"id":"vKwoFfz2lzVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_from_disk\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# loads duplicates dataset\n","duplicates_ds = load_from_disk(\"cleaned_data/duplicates_birds\")\n","\n","# displays first 20 images\n","num_images = min(20, len(duplicates_ds))\n","plt.figure(figsize=(15, 12))\n","\n","for i in range(num_images):\n","    img = duplicates_ds[i][\"image\"]\n","    plt.subplot(4, 5, i+1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.title(f\"Label: {duplicates_ds[i].get('label', 'N/A')}\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"zxVHKc0Pl2gG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_from_disk\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# loads cleaned dataset\n","duplicates_ds = load_from_disk(\"cleaned_data/cleaned_birds\")\n","\n","# displays first 20 images\n","num_images = min(20, len(duplicates_ds))\n","plt.figure(figsize=(15, 12))\n","\n","for i in range(num_images):\n","    img = duplicates_ds[i][\"image\"]\n","    plt.subplot(4, 5, i+1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.title(f\"Label: {duplicates_ds[i].get('label', 'N/A')}\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"N_1Rxu8xapX2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#class imbalances\n","#making a dictionary of species names and how many pictures there are of each species to filter through later\n","#only done for training data since we are just training the model\n","\n","#print(ds['train'].features) #dictionary of names from the label\n","from collections import Counter #counts how many there are of each label\n","values=ds[\"train\"][\"label\"]\n","counts=Counter(values)\n","#print(counts) #how much there are of each value, key associated with a certain species\n","labels=ds[\"train\"].features[\"label\"].names #getting the species names\n","countsSpecies={labels[i]: c for i, c in counts.items()} #makes a dictionary of the species and how many images there are in alphabetical order\n","#print(countsSpecies)"],"metadata":{"id":"AAcC_H5Wzv01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#assigning weights to different classes, best for datasets with medium levels of imbalance (eg. here)\n","#using cross entropy to have the model weigh classes with less samples more\n","#undersampling is not preferred due to the risk of permanently removing important data from the dataset\n","\n","#for PyTorch\n","import torch\n","\n","classCounts=torch.tensor([counts[i] for i in range(len(labels))], dtype=torch.float) #creates a list of how many there are per species alphabetically\n","\n","n=classCounts.sum() #formula for computing the class weights\n","c=len(classCounts)\n","classWeights=n/(c*classCounts)\n","#print(classWeights)"],"metadata":{"id":"DJZIe-9pz1uQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#defining the loss function with these weights to properly evaluate the accuracy after training\n","import torch.nn as nn\n","criterion=nn.CrossEntropyLoss(weight=classWeights)\n","#all of the above goes before creating the model and training it\n","#with this, running the loss function weighs mistakes more heavily on the species with less pictures than speices with more pictures"],"metadata":{"id":"mKGotoG0z4DX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EfficientNet model training starts here\n","!pip install timm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import timm\n","from tqdm import tqdm\n","import numpy as np\n","from torchvision import transforms"],"metadata":{"id":"2DPc5G8TiIoZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fix class weights (class 381 is empty)\n","\n","from collections import Counter\n","\n","labeles = ds['train'].features['label'].names\n","label_values = ds['train']['label']\n","counts = Counter(label_values)\n","\n","classCounts = torch.tensor([counts.get(i,0) for i in range(len(labels))], dtype=torch.float)\n","print(f'original classcounts: {classCounts[381]}')\n","\n","classCounts[classCounts == 0] = 1\n","print(f'fixed classcounts: {classCounts[381]}')\n","\n","n = classCounts.sum()\n","c = len(labels)\n","classWeights = n / (c * classCounts)\n","\n","print(f'Any infinite weights: {torch.isinf(classWeights).any()}')"],"metadata":{"id":"1Oi9MuYtGYid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# filter out class 381 completely (has 0 training images, but has images in validation set)\n","\n","print(\"Original dataset sizes:\")\n","print(f\"  Train: {len(ds['train'])}\")\n","print(f\"  Validation: {len(ds['validation'])}\")\n","print(f\"  Test: {len(ds['test'])}\")\n","\n","# Filter out class 381 from all splits\n","ds['train'] = ds['train'].filter(lambda x: x['label'] != 381)\n","ds['validation'] = ds['validation'].filter(lambda x: x['label'] != 381)\n","ds['test'] = ds['test'].filter(lambda x: x['label'] != 381)\n","\n","print(\"\\nAfter removing PARAKEET AUKLET (class 381):\")\n","print(f\"  Train: {len(ds['train'])} (-{84635 - len(ds['train'])} images)\")\n","print(f\"  Validation: {len(ds['validation'])} (-{2625 - len(ds['validation'])} images)\")\n","print(f\"  Test: {len(ds['test'])} (-{2625 - len(ds['test'])} images)\")\n","print(\"\\n✓ Class 381 removed from all splits\")"],"metadata":{"id":"eeRhWB7BBzXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert dataset to pytorch format\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406], # use predefined imagenet values\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","\n","# custom class\n","class BirdDataset(torch.utils.data.Dataset):\n","    def __init__(self, hf_dataset, transform=None):\n","      self.dataset = hf_dataset\n","      self.transform = transform\n","\n","    def __len__(self):\n","      return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","      item = self.dataset[idx]\n","      image = item['image']\n","      label = item['label']\n","      if self.transform:\n","        image = self.transform(image)\n","      return image, label\n","\n","train_dataset = BirdDataset(ds['train'], transform=transform)\n","val_dataset = BirdDataset(ds['validation'], transform=transform)\n","test_dataset = BirdDataset(ds['test'], transform=transform)\n","\n","print(f\"Train size: {len(train_dataset)}\")\n","print(f\"Validation size: {len(val_dataset)}\")\n","print(f\"Test size: {len(test_dataset)}\")"],"metadata":{"id":"N1zRjC1liKUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create dataloaders (help us batch process and randomize order)\n","\n","BATCH_SIZE  = 32\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=2\n",")\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=2\n",")\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=2\n",")\n","print(f\"Number of training batches: {len(train_loader)}\")"],"metadata":{"id":"o0OVy3FEBPTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load model\n","\n","model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=526)\n","device = torch.device('cuda')\n","model = model.to(device)\n","print(f\"Using device: {device}\")\n","print(f\"Model loaded with {sum(p.numel() for p in model.parameters())} parameters\")"],"metadata":{"id":"tnlgZe-pHWKI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set up training weights\n","\n","classWeights = classWeights.to(device)\n","criterion = nn.CrossEntropyLoss(weight=classWeights)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='min',\n","    factor=0.5,\n","    patience=2,\n",")"],"metadata":{"id":"JNBVQ4WSJJmS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training definitions\n","\n","def train_one_epoch(model, train_loader, criterion, optimizer, device):\n","  model.train()\n","  running_loss = 0.0\n","  correct = 0\n","  total = 0\n","\n","  for images, labels in tqdm(train_loader, desc=\"training\"):\n","      images, labels = images.to(device), labels.to(device)\n","\n","      optimizer.zero_grad()\n","      outputs = model(images)\n","      loss = criterion(outputs, labels)\n","\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      _, predicted = outputs.max(1)\n","      total += labels.size(0)\n","      correct += predicted.eq(labels).sum().item()\n","\n","  epoch_loss = running_loss/len(train_loader)\n","  epoch_acc = 100. * correct / total\n","  return epoch_loss, epoch_acc\n","\n","def validate(model, val_loader, criterion, device):\n","  model.eval()\n","  running_loss = 0.0\n","  correct = 0\n","  total = 0\n","\n","  with torch.no_grad():\n","      for images, labels in tqdm(val_loader, desc=\"validation\"):\n","          images, labels = images.to(device), labels.to(device)\n","          outputs = model(images)\n","          loss = criterion(outputs, labels)\n","          running_loss += loss.item()\n","          _, predicted = outputs.max(1)\n","          total += labels.size(0)\n","          correct += predicted.eq(labels).sum().item()\n","\n","      epoch_loss = running_loss / len(val_loader)\n","      epoch_acc = 100. * correct / total\n","      return epoch_loss, epoch_acc\n","\n"],"metadata":{"id":"q5AitLwHLMRd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run training\n","\n","NUM_EPOCHS = 10\n","best_val_acc = 0.0\n","print('Starting training: \\n')\n","for epoch in range(NUM_EPOCHS):\n","  print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n","\n","  train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n","  print(f\"Train loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n","\n","  val_loss, val_acc = validate(model, val_loader, criterion, device)\n","  print(f\"Val loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n","\n","  scheduler.step(val_loss)\n","\n","  if val_acc > best_val_acc:\n","    best_val_acc = val_acc\n","    torch.save(model.state_dict(), 'best_efficientnet_b0_birds.pth')\n","    print(f\"Training complete, best val accuracy: {best_val_acc:.2f}%\")"],"metadata":{"id":"3jQpxdLURF9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analyze results\n","\n","from collections import Counter\n","from tqdm import tqdm\n","\n","def analyze_confusion(dataset, top_n=15):\n","    # find which species pairs get confused most often\n","\n","    print(\"Analyzing errors on validation set...\")\n","    model.eval()\n","\n","    confusion_pairs = []\n","\n","    with torch.no_grad():\n","        for idx in tqdm(range(len(dataset))):\n","            image, true_label = dataset[idx]\n","            image = image.unsqueeze(0).to(device)\n","\n","            output = model(image)\n","            _, predicted = output.max(1)\n","            pred_label = predicted.item()\n","\n","            # only track errors\n","            if pred_label != true_label:\n","                confusion_pairs.append((true_label, pred_label))\n","\n","    # count most common confusions\n","    confusion_counts = Counter(confusion_pairs)\n","    top_confusions = confusion_counts.most_common(top_n)\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"TOP {top_n} MOST CONFUSED SPECIES PAIRS\")\n","    print(f\"{'='*80}\\n\")\n","\n","    for i, ((true_idx, pred_idx), count) in enumerate(top_confusions, 1):\n","        true_name = labels[true_idx]\n","        pred_name = labels[pred_idx]\n","        print(f\"{i}. {count} times: '{true_name}' → predicted as → '{pred_name}'\")\n","\n","    return top_confusions\n","\n","top_errors = analyze_confusion(val_dataset, top_n=15)"],"metadata":{"id":"uYPiPp8B3s5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visually show species that were confused\n","\n","def show_top_confusions_comparison(dataset, top_errors, num_pairs=3):\n","\n","    fig, axes = plt.subplots(num_pairs, 2, figsize=(12, 5*num_pairs))\n","\n","    for pair_idx in range(num_pairs):\n","        true_idx, pred_idx = top_errors[pair_idx][0]\n","        count = top_errors[pair_idx][1]\n","\n","        true_name = labels[true_idx]\n","        pred_name = labels[pred_idx]\n","\n","        print(f\"\\n{'='*60}\")\n","        print(f\"CONFUSION #{pair_idx+1}: {count} times\")\n","        print(f\"TRUTH: {true_name}\")\n","        print(f\"MODEL PREDICTED: {pred_name}\")\n","        print(f\"{'='*60}\")\n","\n","        # find one example of this confusion\n","        found = False\n","        model.eval()\n","        with torch.no_grad():\n","            for idx in range(len(dataset)):\n","                image, label = dataset[idx]\n","                if label == true_idx:\n","                    image_batch = image.unsqueeze(0).to(device)\n","                    output = model(image_batch)\n","                    _, predicted = output.max(1)\n","\n","                    if predicted.item() == pred_idx:\n","                        confused_image = dataset.dataset[idx]['image']\n","                        found = True\n","                        break\n","\n","        if not found:\n","            print(\"No example found for this pair\")\n","            continue\n","\n","        # LEFT: Show what it actually is (with a correct example)\n","        ax_left = axes[pair_idx, 0] if num_pairs > 1 else axes[0]\n","\n","        # find a correct example of the true species\n","        correct_example = None\n","        with torch.no_grad():\n","            for idx in range(len(dataset)):\n","                image, label = dataset[idx]\n","                if label == true_idx:\n","                    image_batch = image.unsqueeze(0).to(device)\n","                    output = model(image_batch)\n","                    _, predicted = output.max(1)\n","\n","                    if predicted.item() == true_idx:\n","                        correct_example = dataset.dataset[idx]['image']\n","                        break\n","\n","        if correct_example is not None:\n","            ax_left.imshow(correct_example)\n","        else:\n","            ax_left.imshow(confused_image)\n","\n","        ax_left.axis('off')\n","        ax_left.set_title(f\"✓ {true_name}\\n(should've guessed)\",\n","                         fontsize=12, color='green', weight='bold', pad=10)\n","        ax_left.spines['top'].set_color('green')\n","        ax_left.spines['bottom'].set_color('green')\n","        ax_left.spines['left'].set_color('green')\n","        ax_left.spines['right'].set_color('green')\n","        ax_left.spines['top'].set_linewidth(5)\n","        ax_left.spines['bottom'].set_linewidth(5)\n","        ax_left.spines['left'].set_linewidth(5)\n","        ax_left.spines['right'].set_linewidth(5)\n","\n","        # RIGHT: Show what model wrongly thinks it is\n","        ax_right = axes[pair_idx, 1] if num_pairs > 1 else axes[1]\n","        ax_right.imshow(confused_image)\n","        ax_right.axis('off')\n","        ax_right.set_title(f\"✗ MODEL SAYS: {pred_name}\\n(wrong guess)\",\n","                          fontsize=12, color='red', weight='bold', pad=10)\n","        ax_right.spines['top'].set_color('red')\n","        ax_right.spines['bottom'].set_color('red')\n","        ax_right.spines['left'].set_color('red')\n","        ax_right.spines['right'].set_color('red')\n","        ax_right.spines['top'].set_linewidth(5)\n","        ax_right.spines['bottom'].set_linewidth(5)\n","        ax_right.spines['left'].set_linewidth(5)\n","        ax_right.spines['right'].set_linewidth(5)\n","\n","    plt.tight_layout()\n","    plt.savefig('top_3_confusions.png', dpi=150, bbox_inches='tight')\n","    plt.show()\n","    print(\"\\n saved as 'top_3_confusions.png'\")\n","\n","show_top_confusions_comparison(val_dataset, top_errors, num_pairs=3)"],"metadata":{"id":"j5RawHf_3vh1"},"execution_count":null,"outputs":[]}]}